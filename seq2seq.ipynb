{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "import pickle\n",
    "from torch import Tensor\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import pandas as pd\n",
    "\n",
    "from transformer import Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"D:\\\\translation_french_english\\\\archive\\\\en-fr.csv\").sample(frac=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4504075\n",
      "4504061\n"
     ]
    }
   ],
   "source": [
    "print(len(df))\n",
    "df = df.dropna(axis=0)\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>en</th>\n",
       "      <th>fr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6274489</th>\n",
       "      <td>5.5 A contributor subject to the CSC special p...</td>\n",
       "      <td>5.5 Un cotisant assujetti au régime spécial SC...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4037334</th>\n",
       "      <td>The Chief and Council and the health authority...</td>\n",
       "      <td>Le Chef et le Conseil et l’autorité de santé s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12641160</th>\n",
       "      <td>The port city is trade-oriented, has a solid i...</td>\n",
       "      <td>La ville portuaire est axée sur le commerce, e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2698970</th>\n",
       "      <td>The repayment schedule and a list of any condi...</td>\n",
       "      <td>Enfin, le calendrier de remboursement et la li...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14042083</th>\n",
       "      <td>The King, like Metternich and the Austrian Kai...</td>\n",
       "      <td>Le roi, à l’instar de Metternich et des kaiser...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                         en  \\\n",
       "6274489   5.5 A contributor subject to the CSC special p...   \n",
       "4037334   The Chief and Council and the health authority...   \n",
       "12641160  The port city is trade-oriented, has a solid i...   \n",
       "2698970   The repayment schedule and a list of any condi...   \n",
       "14042083  The King, like Metternich and the Austrian Kai...   \n",
       "\n",
       "                                                         fr  \n",
       "6274489   5.5 Un cotisant assujetti au régime spécial SC...  \n",
       "4037334   Le Chef et le Conseil et l’autorité de santé s...  \n",
       "12641160  La ville portuaire est axée sur le commerce, e...  \n",
       "2698970   Enfin, le calendrier de remboursement et la li...  \n",
       "14042083  Le roi, à l’instar de Metternich et des kaiser...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_sentences = df['en'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "fr_sentences = df[\"fr\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# device = torch.device(\"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('english-german-both.pkl', 'rb') as f:\n",
    "#     data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normal sentences - array of strings\n",
    "# eng_sentences = data[:, 0]\n",
    "# ger_sentences = data[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(eng_sentences))\n",
    "# print(len(ger_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4504061\n",
      "4504061\n"
     ]
    }
   ],
   "source": [
    "print(len(en_sentences))\n",
    "print(len(fr_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unique_words(sentences):\n",
    "    all_words = []\n",
    "    for sentence in sentences:\n",
    "        all_words.extend(sentence.split(' '))\n",
    "    return set(all_words)\n",
    "\n",
    "def get_number_of_unique_words(sentences):\n",
    "    return len(get_unique_words(sentences))\n",
    "\n",
    "def sentence_to_indexes(sentence, mapper):\n",
    "    sentences = sentence.split(' ')\n",
    "    indexes = list(map(lambda x: mapper[x], sentences))\n",
    "    return indexes\n",
    "\n",
    "def indexes_to_sentence(indexes, mapper):\n",
    "    words = list(map(lambda x: mapper[x], indexes))\n",
    "    return ' '.join(words)\n",
    "\n",
    "def pad_sequence(sequence, max_len):\n",
    "    sequence = [1] + sequence + [2]\n",
    "    \n",
    "    while len(sequence) < max_len:\n",
    "        sequence.append(0)\n",
    "\n",
    "    return sequence[:max_len]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD_TOKEN_IDX = 0\n",
    "SOS_TOKEN_IDX = 1\n",
    "EOS_TOKEN_IDX = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_words = sorted(get_unique_words(en_sentences))\n",
    "ger_words = sorted(get_unique_words(fr_sentences))\n",
    "\n",
    "index_to_word_eng = {i: word for i, word in enumerate(eng_words, start=3)}\n",
    "index_to_word_eng[0] = \"<PAD>\"\n",
    "index_to_word_eng[1] = \"<SOS>\"\n",
    "index_to_word_eng[2] = \"<EOS>\"\n",
    "word_to_index_eng = {word: i for i, word in enumerate(eng_words, start=3)}\n",
    "word_to_index_eng[\"<PAD>\"] = 0\n",
    "word_to_index_eng[\"<SOS>\"] = 1\n",
    "word_to_index_eng[\"<EOS>\"] = 2\n",
    "\n",
    "index_to_word_ger = {i: word for i, word in enumerate(ger_words, start=3)}\n",
    "index_to_word_ger[0] = \"<PAD>\"\n",
    "index_to_word_ger[1] = \"<SOS>\"\n",
    "index_to_word_ger[2] = \"<EOS>\"\n",
    "word_to_index_ger = {word: i for i, word in enumerate(ger_words, start=3)}\n",
    "word_to_index_ger[\"<PAD>\"] = 0\n",
    "word_to_index_ger[\"<SOS>\"] = 1\n",
    "word_to_index_ger[\"<EOS>\"] = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_sentences_indexes = list(map(lambda x: sentence_to_indexes(x, mapper=word_to_index_eng), en_sentences))\n",
    "ger_sentences_indexes = list(map(lambda x: sentence_to_indexes(x, mapper=word_to_index_ger), fr_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(max(eng_sentences_indexes, key=len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(max(ger_sentences_indexes, key=len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_sentences_indexes = list(map(lambda x: pad_sequence(x, max_len=20), eng_sentences_indexes))\n",
    "ger_sentences_indexes = list(map(lambda x: pad_sequence(x, max_len=20), ger_sentences_indexes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_vocab_size = len(set([item for sublist in eng_sentences_indexes for item in sublist]))\n",
    "trg_vocab_size = len(set([item for sublist in ger_sentences_indexes for item in sublist]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, source, target):\n",
    "        assert len(source) == len(target)\n",
    "        self.source = source\n",
    "        self.target = target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.target)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.source[idx], self.target[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    source_sequences, target_sequences = zip(*batch)\n",
    "    \n",
    "    source_sequences = [torch.tensor(seq) for seq in source_sequences]\n",
    "    target_sequences = [torch.tensor(seq) for seq in target_sequences]\n",
    "    \n",
    "    return torch.stack(source_sequences), torch.stack(target_sequences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TranslationDataset(source=eng_sentences_indexes, target=ger_sentences_indexes)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Transformer(\n",
    "    src_pad_idx=0, \n",
    "    trg_pad_idx=0, \n",
    "    device=device, \n",
    "    d_model=512, \n",
    "    heads=8, \n",
    "    dropout=0.1, \n",
    "    max_len=20, \n",
    "    num_layers=6, \n",
    "    src_vocab_size=src_vocab_size, \n",
    "    trg_vocab_size=trg_vocab_size\n",
    "    ).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=3e-4)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(10):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_batches = len(train_dataloader)\n",
    "    \n",
    "    for src, trg in train_dataloader:\n",
    "        src, trg = src.to(device), trg.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        output = model(src, trg[:, :-1])\n",
    "        output = output.reshape(-1, output.shape[2])\n",
    "        trg = trg[:, 1:].reshape(-1)\n",
    "        \n",
    "        # Obliczenie straty\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(output, trg)\n",
    "        loss.backward()\n",
    "        # nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    # Średnia strata z epoki\n",
    "    avg_loss = total_loss / total_batches\n",
    "    print(f\"Epoch: {epoch+1}, Average Loss: {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_sentences[4444]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "test_sentence = eng_sentences[4444]\n",
    "tokenized_text = sentence_to_indexes(test_sentence, mapper=word_to_index_eng)\n",
    "tokenized_text = pad_sequence(tokenized_text, max_len=10)\n",
    "\n",
    "sentence_tensor = torch.tensor(tokenized_text).unsqueeze(0).to(device)\n",
    "print(sentence_tensor.shape)\n",
    "\n",
    "outputs = [1]\n",
    "for i in range(10):\n",
    "    trg_tensor = torch.LongTensor(outputs).unsqueeze(1).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(sentence_tensor, trg_tensor)\n",
    "    \n",
    "    best_guess = output.argmax(2)[-1, :].item()\n",
    "    outputs.append(best_guess)\n",
    "\n",
    "print(indexes_to_sentence(outputs, mapper=index_to_word_ger))\n",
    "print(outputs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
